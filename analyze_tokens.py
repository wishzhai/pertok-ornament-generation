#!/usr/bin/env python3
"""
åˆ†æç”Ÿæˆçš„tokenåºåˆ—ï¼Œç‰¹åˆ«å…³æ³¨TimeShift tokençš„åˆ†å¸ƒ
"""

import torch
from working_pertok_config import create_working_tokenizer
from fixed_pertok_decoder import FixedPerTokDecoder
from inference import OrnamentInferenceEngine


def analyze_token_sequence(tokens, tokenizer, decoder):
    """è¯¦ç»†åˆ†ætokenåºåˆ—"""
    vocab = tokenizer.vocab
    id_to_str = {v: k for k, v in vocab.items()}

    print("=" * 80)
    print("ğŸ” TOKENåºåˆ—è¯¦ç»†åˆ†æ")
    print("=" * 80)

    # ç»Ÿè®¡å„ç±»token
    token_counts = {
        'pitch': 0,
        'velocity': 0,
        'duration': 0,
        'timeshift': 0,
        'microtiming': 0,
        'timesig': 0,
        'special': 0,
        'other': 0
    }

    timeshift_values = []
    duration_values = []

    print("\nğŸ“Š Tokenåºåˆ—åˆ†æ:")
    print(f"{'ä½ç½®':<4} {'Token ID':<8} {'Tokenç±»å‹':<15} {'å€¼':<20} {'æè¿°'}")
    print("-" * 80)

    for i, token_id in enumerate(tokens):
        token_str = id_to_str.get(token_id, f'UNK_{token_id}')

        # åˆ†ç±»token
        if token_str.startswith('Pitch_'):
            token_counts['pitch'] += 1
            pitch_val = token_str.replace('Pitch_', '')
            print(f"{i:<4} {token_id:<8} {'Pitch':<15} {pitch_val:<20} MIDIéŸ³ç¬¦")
        elif token_str.startswith('Velocity_'):
            token_counts['velocity'] += 1
            vel_val = token_str.replace('Velocity_', '')
            print(f"{i:<4} {token_id:<8} {'Velocity':<15} {vel_val:<20} åŠ›åº¦")
        elif token_str.startswith('Duration_'):
            token_counts['duration'] += 1
            dur_val = token_str.replace('Duration_', '')
            duration_values.append(dur_val)
            print(f"{i:<4} {token_id:<8} {'Duration':<15} {dur_val:<20} æ—¶å€¼")
        elif token_str.startswith('TimeShift_'):
            token_counts['timeshift'] += 1
            shift_val = token_str.replace('TimeShift_', '')
            timeshift_values.append(shift_val)
            print(f"{i:<4} {token_id:<8} {'TimeShift':<15} {shift_val:<20} æ—¶é—´åç§»")
        elif token_str.startswith('MicroTiming_'):
            token_counts['microtiming'] += 1
            micro_val = token_str.replace('MicroTiming_', '')
            print(f"{i:<4} {token_id:<8} {'MicroTiming':<15} {micro_val:<20} å¾®æ—¶åº")
        elif token_str.startswith('TimeSig_'):
            token_counts['timesig'] += 1
            print(f"{i:<4} {token_id:<8} {'TimeSig':<15} {token_str.replace('TimeSig_', ''):<20} æ‹å·")
        elif token_str.endswith('_None'):
            token_counts['special'] += 1
            print(f"{i:<4} {token_id:<8} {'Special':<15} {token_str:<20} ç‰¹æ®Štoken")
        else:
            token_counts['other'] += 1
            print(f"{i:<4} {token_id:<8} {'Other':<15} {token_str:<20} å…¶ä»–")

    print("\n" + "=" * 80)
    print("ğŸ“ˆ TOKENç»Ÿè®¡")
    print("=" * 80)

    total_tokens = len(tokens)
    for token_type, count in token_counts.items():
        percentage = (count / total_tokens) * 100 if total_tokens > 0 else 0
        print(f"{token_type.upper():<15}: {count:<4} ({percentage:>5.1f}%)")

    print(f"\næ€»tokenæ•°: {total_tokens}")

    # åˆ†æTimeShiftåˆ†å¸ƒ
    if timeshift_values:
        print("\n" + "=" * 80)
        print("â±ï¸  TIMESHIFTè¯¦ç»†åˆ†æ")
        print("=" * 80)

        # è½¬æ¢æ—¶é—´å€¼ä¸ºæµ®ç‚¹æ•°
        shift_floats = []
        for val in timeshift_values:
            try:
                # å¤„ç†PerTokæ ¼å¼ "1.0.320" -> æå– "1.0"
                if '.' in val:
                    parts = val.split('.')
                    if len(parts) >= 2:
                        shift_floats.append(float(f"{parts[0]}.{parts[1]}"))
                    else:
                        shift_floats.append(float(val))
                else:
                    shift_floats.append(float(val))
            except Exception:
                continue

        if shift_floats:
            print("TimeShiftå€¼åˆ†å¸ƒ:")
            print(f"  æœ€å°å€¼: {min(shift_floats):.3f} beats")
            print(f"  æœ€å¤§å€¼: {max(shift_floats):.3f} beats")
            print(f"  å¹³å‡å€¼: {sum(shift_floats)/len(shift_floats):.3f} beats")

            # åˆ†æåœé¡¿é—®é¢˜
            short_shifts = [s for s in shift_floats if s < 0.25]  # å°äº1/4æ‹
            medium_shifts = [s for s in shift_floats if 0.25 <= s < 1.0]  # 1/4æ‹åˆ°1æ‹
            long_shifts = [s for s in shift_floats if s >= 1.0]  # å¤§äºç­‰äº1æ‹

            print("\nåœé¡¿åˆ†æ:")
            print(f"  çŸ­åœé¡¿ (<0.25æ‹): {len(short_shifts)} ä¸ª ({len(short_shifts)/len(shift_floats)*100:.1f}%)")
            print(f"  ä¸­ç­‰åœé¡¿ (0.25-1æ‹): {len(medium_shifts)} ä¸ª ({len(medium_shifts)/len(shift_floats)*100:.1f}%)")
            print(f"  é•¿åœé¡¿ (â‰¥1æ‹): {len(long_shifts)} ä¸ª ({len(long_shifts)/len(shift_floats)*100:.1f}%)")

            if short_shifts:
                print(f"  çŸ­åœé¡¿å€¼: {[f'{s:.3f}' for s in short_shifts[:10]]}")
            if long_shifts:
                print(f"  é•¿åœé¡¿å€¼: {[f'{s:.3f}' for s in long_shifts[:10]]}")

    # åˆ†æDurationåˆ†å¸ƒ
    if duration_values:
        print("\n" + "=" * 80)
        print("ğŸµ DURATIONè¯¦ç»†åˆ†æ")
        print("=" * 80)

        dur_floats = []
        for val in duration_values:
            try:
                if '.' in val:
                    parts = val.split('.')
                    if len(parts) >= 2:
                        dur_floats.append(float(f"{parts[0]}.{parts[1]}"))
                    else:
                        dur_floats.append(float(val))
                else:
                    dur_floats.append(float(val))
            except Exception:
                continue

        if dur_floats:
            print("Durationå€¼åˆ†å¸ƒ:")
            print(f"  æœ€å°å€¼: {min(dur_floats):.3f} beats")
            print(f"  æœ€å¤§å€¼: {max(dur_floats):.3f} beats")
            print(f"  å¹³å‡å€¼: {sum(dur_floats)/len(dur_floats):.3f} beats")

            short_durs = [d for d in dur_floats if d < 0.5]  # å°äºåŠæ‹
            medium_durs = [d for d in dur_floats if 0.5 <= d < 1.0]  # åŠæ‹åˆ°1æ‹
            long_durs = [d for d in dur_floats if d >= 1.0]  # å¤§äºç­‰äº1æ‹

            print("\næ—¶å€¼åˆ†æ:")
            print(f"  çŸ­æ—¶å€¼ (<0.5æ‹): {len(short_durs)} ä¸ª ({len(short_durs)/len(dur_floats)*100:.1f}%)")
            print(f"  ä¸­ç­‰æ—¶å€¼ (0.5-1æ‹): {len(medium_durs)} ä¸ª ({len(medium_durs)/len(dur_floats)*100:.1f}%)")
            print(f"  é•¿æ—¶å€¼ (â‰¥1æ‹): {len(long_durs)} ä¸ª ({len(long_durs)/len(dur_floats)*100:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹åˆ†ætokenåºåˆ—...")

    # åˆå§‹åŒ–ç»„ä»¶
    tokenizer = create_working_tokenizer()
    decoder = FixedPerTokDecoder(tokenizer)

    # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    model_path = 'checkpoints_ornament_aware/best_ornament_aware_model.pth'

    try:
        engine = OrnamentInferenceEngine(model_path, device='cpu')

        # ç¼–ç è¾“å…¥MIDI
        print("\n1ï¸âƒ£ ç¼–ç è¾“å…¥MIDI...")
        input_tokens = engine.encode_midi('demo_input.mid')
        if input_tokens:
            print(f"âœ… è¾“å…¥ç¼–ç æˆåŠŸ: {len(input_tokens)} tokens")

            # åˆ†æè¾“å…¥åºåˆ—
            print("\nğŸ“Š åˆ†æè¾“å…¥åºåˆ—:")
            analyze_token_sequence(input_tokens, tokenizer, decoder)

            # ç”Ÿæˆè£…é¥°éŸ³
            print("\n2ï¸âƒ£ ç”Ÿæˆè£…é¥°éŸ³...")
            generated_tokens = engine.generate_ornaments(
                input_tokens,
                temperature=1.0,
                top_k=50,
                top_p=0.9
            )

            if generated_tokens:
                print(f"âœ… ç”ŸæˆæˆåŠŸ: {len(generated_tokens)} tokens")

                # åˆ†æç”Ÿæˆåºåˆ—
                print("\nğŸ“Š åˆ†æç”Ÿæˆåºåˆ—:")
                analyze_token_sequence(generated_tokens, tokenizer, decoder)

                # å¯¹æ¯”åˆ†æ
                print("\n" + "=" * 80)
                print("ğŸ”„ è¾“å…¥vsè¾“å‡ºå¯¹æ¯”")
                print("=" * 80)

                vocab = tokenizer.vocab
                id_to_str = {v: k for k, v in vocab.items()}

                input_timeshifts = sum(1 for t in input_tokens if id_to_str.get(t, '').startswith('TimeShift_'))
                output_timeshifts = sum(1 for t in generated_tokens if id_to_str.get(t, '').startswith('TimeShift_'))

                print("TimeShift tokens:")
                print(f"  è¾“å…¥: {input_timeshifts} ä¸ª")
                print(f"  è¾“å‡º: {output_timeshifts} ä¸ª")
                print(f"  å¢åŠ : {output_timeshifts - input_timeshifts} ä¸ª")

                if len(input_tokens) > 0 and len(generated_tokens) > 0:
                    input_ratio = input_timeshifts / len(input_tokens) * 100
                    output_ratio = output_timeshifts / len(generated_tokens) * 100
                    print(f"  è¾“å…¥æ¯”ä¾‹: {input_ratio:.1f}%")
                    print(f"  è¾“å‡ºæ¯”ä¾‹: {output_ratio:.1f}%")
                    print(f"  æ¯”ä¾‹å˜åŒ–: {output_ratio - input_ratio:+.1f}%")

        else:
            print("âŒ è¾“å…¥ç¼–ç å¤±è´¥")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


