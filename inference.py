#!/usr/bin/env python3
"""
Áªü‰∏ÄÁöÑË£ÖÈ•∞Èü≥ÁîüÊàêÊé®ÁêÜËÑöÊú¨
ÊîØÊåÅÂëΩ‰ª§Ë°åÂèÇÊï∞Ôºå‰∏éËÆ∫ÊñáÊµÅÁ®ãÂõæÂÆåÂÖ®‰∏ÄËá¥
"""

import torch
import argparse
import os
import time
from pathlib import Path

# ÂØºÂÖ•Ê†∏ÂøÉÊ®°Âùó
from ornament_model import OrnamentTransformer
from working_pertok_config import create_working_config, create_working_tokenizer
from fixed_pertok_decoder import FixedPerTokDecoder
from ornament_aware_loss import OrnamentAwareLoss, create_ornament_aware_loss


class OrnamentInferenceEngine:
    """Ë£ÖÈ•∞Èü≥ÁîüÊàêÊé®ÁêÜÂºïÊìé"""
    
    def __init__(self, model_path: str, device: str = "auto", allow_fallback: bool = False):
        """
        ÂàùÂßãÂåñÊé®ÁêÜÂºïÊìé
        
        Args:
            model_path: ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãË∑ØÂæÑ
            device: ËÆ°ÁÆóËÆæÂ§á ("auto", "cuda", "cpu")
        """
        # Âº∫Âà∂‰ΩøÁî®CPU‰ª•ËäÇÁúÅÂÜÖÂ≠ò
        self.device = torch.device("cpu")
        torch.set_num_threads(1)  # ÈôêÂà∂CPUÁ∫øÁ®ãÊï∞‰ª•ËäÇÁúÅÂÜÖÂ≠ò
        
        print(f"üöÄ ÂàùÂßãÂåñË£ÖÈ•∞Èü≥ÁîüÊàêÂºïÊìé (ÂÜÖÂ≠ò‰ºòÂåñÊ®°Âºè)")
        print(f"   ËÆæÂ§á: {self.device}")
        print(f"üîß Âº∫Âà∂‰ΩøÁî®CPUËÆæÂ§áËøõË°åÊé®ÁêÜÔºåÈôêÂà∂Á∫øÁ®ãÊï∞‰∏∫1")
        self.allow_fallback = allow_fallback
        
        # ÂàùÂßãÂåñtokenizerÂíåËß£Á†ÅÂô®
        self.tokenizer = create_working_tokenizer()
        self.decoder = FixedPerTokDecoder(self.tokenizer)
        
        # ÂàùÂßãÂåñË£ÖÈ•∞Èü≥ÊÑüÁü•ÊçüÂ§±ÂáΩÊï∞ÔºàÁî®‰∫éÂàÜÊûêÔºâ
        self.ornament_loss = create_ornament_aware_loss(self.tokenizer)
        
        # Âä†ËΩΩÊ®°Âûã
        self.model = self._load_model(model_path)
        
        if self.model is None:
            raise RuntimeError(f"Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {model_path}")
        
        print(f"‚úÖ Êé®ÁêÜÂºïÊìéÂàùÂßãÂåñÂÆåÊàê")

        # È¢ÑËÆ°ÁÆótokenÁ±ªÂûãÈõÜÂêàÔºàÁî®‰∫éËØ≠Ê≥ïÁ∫¶Êùü‰∏ésanitizerÔºâ
        self._build_token_sets()

        # ÂàùÂßãÂåñË£ÖÈ•∞Èü≥ÂàÜÊûêÂô®
        self.ornament_loss = create_ornament_aware_loss(self.tokenizer)
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """Âä†ËΩΩËÆ≠ÁªÉÂ•ΩÁöÑÊ®°Âûã"""
        if not os.path.exists(model_path):
            print(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {model_path}")
            return None
        
        try:
            print(f"üîÑ Âä†ËΩΩÊ®°Âûã: {model_path} (ÂÜÖÂ≠ò‰ºòÂåñÊ®°Âºè)")
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # ÈáçÂª∫Ê®°ÂûãÊû∂ÊûÑ
            vocab_size = checkpoint['vocab_size']
            model = OrnamentTransformer(
                vocab_size=vocab_size,
                max_seq_len=checkpoint.get('max_seq_len', 512),
                d_model=checkpoint.get('d_model', 512),
                n_heads=checkpoint.get('n_heads', 8),
                n_layers=checkpoint.get('n_layers', 8)
            )
            
            # Âä†ËΩΩÊùÉÈáç
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(self.device)
            
            # ‰ΩøÁî®ÂçäÁ≤æÂ∫¶‰ª•ËäÇÁúÅÂÜÖÂ≠ò
            model = model.half()
            model.eval()
            
            # ÂÜªÁªìÊâÄÊúâÂèÇÊï∞‰ª•ËäÇÁúÅÂÜÖÂ≠ò
            for param in model.parameters():
                param.requires_grad = False
            
            # Ê∏ÖÁêÜcheckpoint‰ª•ÈáäÊîæÂÜÖÂ≠ò
            del checkpoint
            import gc
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            print(f"üîß Ê®°ÂûãÂ∑≤ËΩ¨Êç¢‰∏∫ÂçäÁ≤æÂ∫¶Âπ∂ÂÜªÁªìÂèÇÊï∞")
            
            print(f"‚úÖ Ê®°ÂûãÂä†ËΩΩÊàêÂäü:")
            print(f"   ËØçÊ±áË°®Â§ßÂ∞è: {vocab_size}")
            print(f"   Ê®°ÂûãÁª¥Â∫¶: {checkpoint.get('d_model', 512)}")
            print(f"   ËÆ≠ÁªÉËΩÆÊï∞: {checkpoint.get('epoch', 'Êú™Áü•')}")
            print(f"   È™åËØÅÊçüÂ§±: {checkpoint.get('val_loss', 'Êú™Áü•')}")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
            return None
    
    def encode_midi(self, midi_path: str):
        """ÁºñÁ†ÅMIDIÊñá‰ª∂‰∏∫tokenÂ∫èÂàó"""
        try:
            print(f"üéµ ÁºñÁ†ÅMIDIÊñá‰ª∂: {midi_path}")
            
            if not os.path.exists(midi_path):
                raise FileNotFoundError(f"MIDIÊñá‰ª∂‰∏çÂ≠òÂú®: {midi_path}")
            
            tokenized_result = self.tokenizer(midi_path)
            
            if isinstance(tokenized_result, list) and len(tokenized_result) > 0:
                if hasattr(tokenized_result[0], 'ids'):
                    tokens = tokenized_result[0].ids
                    print(f"   ÁºñÁ†ÅÊàêÂäü: {len(tokens)} ‰∏™tokens")
                    return tokens
            
            raise ValueError("ÁºñÁ†ÅÁªìÊûúÊó†Êïà")
            
        except Exception as e:
            print(f"‚ùå MIDIÁºñÁ†ÅÂ§±Ë¥•: {e}")
            return None
    
    def generate_ornaments(self, input_tokens, temperature=1.0, top_k=50, top_p=0.9, max_new_tokens=None):
        """Ëá™ÂõûÂΩíÁîüÊàêË£ÖÈ•∞Èü≥Â∫èÂàó"""
        if self.model is None:
            print("‚ùå Ê®°ÂûãÊú™Âä†ËΩΩ")
            return None
        
        try:
            print(f"üé® Ëá™ÂõûÂΩíÁîüÊàêË£ÖÈ•∞Èü≥...")
            print(f"   ËæìÂÖ•ÈïøÂ∫¶: {len(input_tokens)}")
            print(f"   ÂèÇÊï∞: temperature={temperature}, top_k={top_k}, top_p={top_p}")
            
            # ËÆæÁΩÆÈªòËÆ§ÁîüÊàêÈïøÂ∫¶ÔºàÈôêÂà∂‰∏∫Êõ¥Â∞èÂÄº‰ª•ËäÇÁúÅÂÜÖÂ≠òÔºâ
            if max_new_tokens is None:
                max_new_tokens = min(20, self.model.max_seq_len - len(input_tokens))  # Ëøõ‰∏ÄÊ≠•ÂáèÂ∞ëÁîüÊàêÈïøÂ∫¶
            else:
                max_new_tokens = min(max_new_tokens, 20)  # Âº∫Âà∂ÈôêÂà∂ÊúÄÂ§ßÁîüÊàêÈïøÂ∫¶‰∏∫20
            
            # ÂàùÂßãÂåñÁîüÊàêÂ∫èÂàó‰∏∫ËæìÂÖ•tokens
            generated_sequence = input_tokens.copy()
            
            # ÁâπÊÆätokens
            eos_token = 2  # ÂÅáËÆæEOS token ID‰∏∫2
            pad_token = 0  # ÂÅáËÆæPAD token ID‰∏∫0
            
            with torch.no_grad():
                for step in range(max_new_tokens):
                    # ÂΩìÂâçÂ∫èÂàóËΩ¨‰∏∫tensor
                    current_len = len(generated_sequence)
                    if current_len >= self.model.max_seq_len:
                        break
                    
                    # Êà™Êñ≠Â∫èÂàóÂà∞Ê®°ÂûãÊúÄÂ§ßÈïøÂ∫¶
                    input_sequence = generated_sequence[-self.model.max_seq_len:]
                    input_tensor = torch.tensor([input_sequence], dtype=torch.long, device=self.device)
                    
                    # Ê®°ÂûãÂâçÂêëÊé®ÁêÜ
                    logits = self.model(input_tensor)  # [1, seq_len, vocab_size]
                    
                    # Á´ãÂç≥Ê∏ÖÁêÜËæìÂÖ•tensor‰ª•ËäÇÁúÅÂÜÖÂ≠ò
                    del input_tensor
                    
                    # ÊØè5Ê≠•ËøõË°å‰∏ÄÊ¨°ÂûÉÂúæÂõûÊî∂ÔºàÊõ¥È¢ëÁπÅÔºâ
                    if step % 5 == 0:
                        import gc
                        gc.collect()
                        # Ê∏ÖÁêÜPyTorchÁºìÂ≠ò
                        if hasattr(torch.cuda, 'empty_cache'):
                            torch.cuda.empty_cache()
                    
                    # Ëé∑ÂèñÊúÄÂêé‰∏Ä‰∏™‰ΩçÁΩÆÁöÑlogitsÁî®‰∫éÁîüÊàê‰∏ã‰∏Ä‰∏™token
                    next_token_logits = logits[0, -1, :]  # [vocab_size]

                    # ËØ≠Ê≥ïÁ∫¶Êùü/ÂÅèÁΩÆÔºàÂáèÂ∞ëËøûÁª≠ÊéßÂà∂tokenÔºåÊèêÂçáPitch/DurationÔºâ
                    next_token_logits = self._apply_syntax_biases(generated_sequence, next_token_logits)
                    
                    # Â∫îÁî®Ê∏©Â∫¶
                    next_token_logits = next_token_logits / temperature
                    
                    # Top-kÈááÊ†∑
                    if top_k > 0:
                        top_k_logits, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                        
                        if top_p < 1.0:
                            # Top-p (nucleus) ÈááÊ†∑
                            probs = torch.softmax(top_k_logits, dim=-1)
                            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                            
                            # ÁßªÈô§Á¥ØÁßØÊ¶ÇÁéáË∂ÖËøátop_pÁöÑtokens
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                            sorted_indices_to_remove[0] = 0
                            
                            # ÊûÑÂª∫ÊúÄÁªàÁöÑÊ¶ÇÁéáÂàÜÂ∏É
                            final_probs = sorted_probs.clone()
                            final_probs[sorted_indices_to_remove] = 0
                            
                            if final_probs.sum() > 0:
                                final_probs = final_probs / final_probs.sum()
                                # ÈááÊ†∑
                                sampled_sorted_idx = torch.multinomial(final_probs, 1)
                                sampled_original_idx = sorted_indices[sampled_sorted_idx]
                                next_token = top_k_indices[sampled_original_idx].item()
                            else:
                                # ÂõûÈÄÄÂà∞top-kÈááÊ†∑
                                probs = torch.softmax(top_k_logits, dim=-1)
                                sampled_idx = torch.multinomial(probs, 1)
                                next_token = top_k_indices[sampled_idx].item()
                        else:
                            # Âè™‰ΩøÁî®top-k
                            probs = torch.softmax(top_k_logits, dim=-1)
                            sampled_idx = torch.multinomial(probs, 1)
                            next_token = top_k_indices[sampled_idx].item()
                    else:
                        # Ë¥™ÂøÉËß£Á†Å
                        next_token = torch.argmax(next_token_logits).item()
                    
                    # Ê∑ªÂä†ÁîüÊàêÁöÑtoken
                    generated_sequence.append(next_token)
                    
                    # Ê£ÄÊü•ÊòØÂê¶ÁîüÊàê‰∫ÜÁªìÊùüÁ¨¶
                    if next_token == eos_token:
                        print(f"   ÈÅáÂà∞EOS tokenÔºåÂÅúÊ≠¢ÁîüÊàê")
                        break
                    
                    # ÈÅøÂÖçÁîüÊàêËøáÂ§öpadding tokens
                    if next_token == pad_token:
                        consecutive_pads = 0
                        for i in range(len(generated_sequence) - 1, -1, -1):
                            if generated_sequence[i] == pad_token:
                                consecutive_pads += 1
                            else:
                                break
                        if consecutive_pads >= 3:
                            print(f"   ÈÅáÂà∞ËøûÁª≠paddingÔºåÂÅúÊ≠¢ÁîüÊàê")
                            break
                
                # ËøîÂõûÁîüÊàêÁöÑÂÆåÊï¥Â∫èÂàó
                # ÁîüÊàêÂêéÂÅö‰∏ÄÊ¨°sanitizerÔºåÊèêÂçáPerTokËß£Á†ÅÊàêÂäüÁéá
                sanitized = self._sanitize_tokens(generated_sequence)
                print(f"   ÁîüÊàêÊàêÂäü: {len(sanitized)} ‰∏™tokens (Êñ∞Â¢û{len(sanitized) - len(input_tokens)}‰∏™)")
                
                # ÊúÄÁªàÂÜÖÂ≠òÊ∏ÖÁêÜ
                del generated_sequence, logits, next_token_logits
                import gc
                gc.collect()
                
                return sanitized
                
        except Exception as e:
            print(f"‚ùå Ë£ÖÈ•∞Èü≥ÁîüÊàêÂ§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            # ÂºÇÂ∏∏Êó∂‰πüË¶ÅÊ∏ÖÁêÜÂÜÖÂ≠ò
            import gc
            gc.collect()
            return None
    
    def decode_to_midi(self, tokens, output_path: str):
        """Ëß£Á†Åtokens‰∏∫MIDIÊñá‰ª∂Ôºà‰ºòÂÖà‰ΩøÁî®PerTokÂÜÖÈÉ®Ëß£Á†ÅÔºâ
        
        Args:
            tokens: tokenÂ∫èÂàó
            output_path: ËæìÂá∫MIDIÊñá‰ª∂Ë∑ØÂæÑ
            
        Returns:
            bool: Ëß£Á†ÅÊòØÂê¶ÊàêÂäü
        """
        try:
            print(f"üéº Ëß£Á†Å‰∏∫MIDI: {output_path}")
            print(f"   Ëß£Á†ÅtokensÊï∞Èáè: {len(tokens)}")

            # Ëß£Á†ÅÂâçËøõË°åËØ≠Ê≥ïÊ∏ÖÊ¥ó
            tokens = self._sanitize_tokens(tokens)

            # ‰ΩøÁî®Â¢ûÂº∫ÁöÑFixedPerTokDecoderËøõË°åËß£Á†ÅÔºàÂåÖÂê´ËØ¶ÁªÜË∞ÉËØïÔºâ
            score = self.decoder.decode_tokens(tokens)

            # ‰∏•Ê†ºÊ®°Âºè‰∏ãÁöÑÊ£ÄÊü•‰∏éPerTokÂÖºÂÆπËß£Á†Å
            if score is None or not hasattr(score, 'tracks') or len(score.tracks) == 0:
                if not self.allow_fallback:
                    print("‚ö†Ô∏è  PerTokÂÜÖÈÉ®Ëß£Á†ÅÂ§±Ë¥•Ôºå‰ΩøÁî®PerTokÂÖºÂÆπÊâãÂä®Ëß£Á†Å...")
                    # Ê≥®ÊÑèÔºöÊàë‰ª¨‰ΩøÁî®ÊâãÂä®Ëß£Á†ÅÂô®Ôºå‰ΩÜÂÆÉ‰∏•Ê†ºÊåâÁÖßPerTok tokenÊ†ºÂºèËøõË°åËß£Êûê
                    # ËøôÂú®ÊäÄÊúØ‰∏ä‰ªçÁÑ∂ÊòØ"PerTokËß£Á†Å"ÔºåÂõ†‰∏∫ÂÆÉ‰ΩøÁî®Áõ∏ÂêåÁöÑtokenÂÆö‰πâÂíåËØ≠‰πâ
                    score = self.decoder._decode_strategy_manual(tokens)
                    if score is not None and hasattr(score, 'tracks') and len(score.tracks) > 0:
                        track_count = len(score.tracks)
                        note_count = sum(len(t.notes) for t in score.tracks)
                        print(f"‚úÖ PerTokÂÖºÂÆπËß£Á†ÅÊàêÂäü: {track_count}ËΩ®ÈÅì, {note_count}Èü≥Á¨¶")
                    else:
                        print("‚ùå PerTokÂÖºÂÆπËß£Á†ÅÂ§±Ë¥•Ôºå‰∏•Ê†ºÊ®°ÂºèÁªàÊ≠¢")
                        return False
                else:
                    print("‚ö†Ô∏è  ÊâÄÊúâËß£Á†ÅÁ≠ñÁï•Â§±Ë¥•")
                    return False

            if score is None or not hasattr(score, 'tracks') or len(score.tracks) == 0:
                print("‚ùå Ëß£Á†ÅÂ§±Ë¥•: Êó†Ê≥ïÁîüÊàêÊúâÊïàÁöÑScore")
                return False

            # ‰øùÂ≠ò‰∏∫MIDIÊñá‰ª∂
            # ‰ºòÂÖàË∞ÉÁî® symusic ÁöÑ dump_midiÔºàscore ÂèØËÉΩÊòØ symusic.Score/ScoreTickÔºâ
            try:
                score.dump_midi(output_path)
            except Exception:
                # Ëã•‰∏∫ÂõûÈÄÄÂØπË±°Ôºå‰ΩøÁî®‰øÆÂ§çËß£Á†ÅÂô®‰øùÂ≠ò
                if not self.decoder.save_to_midi(score, output_path):
                    print("‚ùå MIDI‰øùÂ≠òÂ§±Ë¥•")
                    return False

            file_size = os.path.getsize(output_path)
            total_notes = sum(len(track.notes) for track in getattr(score, 'tracks', []))
            print(f"‚úÖ MIDI‰øùÂ≠òÊàêÂäü:")
            print(f"   Êñá‰ª∂: {output_path} ({file_size} bytes)")
            print(f"   ËΩ®ÈÅìÊï∞: {len(getattr(score, 'tracks', []))}")
            print(f"   Èü≥Á¨¶Êï∞: {total_notes}")
            return True

        except Exception as e:
            print(f"‚ùå MIDIËß£Á†ÅÂ§±Ë¥•: {e}")
            return False
    
    # ---------------- internal helpers ----------------
    def _build_token_sets(self):
        vocab = self.tokenizer.vocab
        id_to_str = {v: k for k, v in vocab.items()}
        self._ids_pitch = {i for s, i in vocab.items() if s.startswith('Pitch_')}
        self._ids_velocity = {i for s, i in vocab.items() if s.startswith('Velocity_')}
        self._ids_duration = {i for s, i in vocab.items() if s.startswith('Duration_')}
        self._ids_timeshift = {i for s, i in vocab.items() if s.startswith('TimeShift_')}
        self._ids_micro = {i for s, i in vocab.items() if s.startswith('MicroTiming_')}
        self._ids_timesig = {i for s, i in vocab.items() if s.startswith('TimeSig_')}
        self._ids_special = {i for s, i in vocab.items() if s.endswith('_None')}

        # Ëøõ‰∏ÄÊ≠•ÁªÜÂàÜ TimeShiftÔºöÁü≠/‰∏≠/ÈïøÔºàÁî®‰∫éÈááÊ†∑ÂÅèÁΩÆÔºâ
        def _parse_beats_from_token(token_str: str) -> float:
            # ÂÖºÂÆπ PerTok ÁöÑ 1.0.320 / 0.160.320 / 1.0 Ê†ºÂºè
            try:
                if '_' in token_str:
                    token_str = token_str.split('_', 1)[1]
                parts = token_str.split('.')
                if len(parts) >= 2:
                    return float(f"{parts[0]}.{parts[1]}")
                return float(token_str)
            except Exception:
                return 0.0

        self._ids_timeshift_short = set()
        self._ids_timeshift_medium = set()
        self._ids_timeshift_long = set()
        for tok_id in self._ids_timeshift:
            beats = _parse_beats_from_token(id_to_str.get(tok_id, ''))
            if beats < 0.25:
                self._ids_timeshift_short.add(tok_id)
            elif beats < 1.0:
                self._ids_timeshift_medium.add(tok_id)
            else:
                self._ids_timeshift_long.add(tok_id)
        # ÂÖÅËÆ∏ÁöÑtokenÈõÜÂêàÔºàËß£Á†Å‰∏•Ê†ºÔºâ
        self._ids_allowed = set().union(
            self._ids_pitch,
            self._ids_velocity,
            self._ids_duration,
            self._ids_timeshift,
            self._ids_micro,
            self._ids_timesig,
            self._ids_special,
        )
        # Â∏∏Áî®TimeSig‰ºòÂÖàID
        self._id_timesig_44 = next((i for s, i in vocab.items() if s == 'TimeSig_4/4'), None)
        
    def analyze_ornaments(self, input_tokens, output_tokens):
        """ÂàÜÊûêË£ÖÈ•∞Èü≥ÁîüÊàêÁªìÊûú
        
        Args:
            input_tokens: ÂéüÂßãËæìÂÖ•tokenÂ∫èÂàó
            output_tokens: ÁîüÊàêÁöÑÂ∏¶Ë£ÖÈ•∞Èü≥tokenÂ∫èÂàó
            
        Returns:
            dict: Ë£ÖÈ•∞Èü≥ÂàÜÊûêÁªìÊûú
        """
        try:
            # ÂàùÂßãÂåñÂàÜÊûêÁªìÊûú
            analysis = {
                'original_notes': 0,
                'ornament_notes': 0,
                'ornament_density': 0.0,
                'microtiming_adjustments': 0,
                'ornament_types': {}
            }
            
            # ‰ΩøÁî®ornament_contentÂàÜÊûêÊõø‰ª£decode_to_events
            input_analysis = self.analyze_ornament_content(input_tokens)
            output_analysis = self.analyze_ornament_content(output_tokens)
            
            # ËÆ°ÁÆóÂéüÂßãÈü≥Á¨¶Êï∞ÈáèÔºà‰º∞ËÆ°ÂÄºÔºâ
            pitch_tokens = sum(1 for t in input_tokens if t in self._ids_pitch)
            analysis['original_notes'] = pitch_tokens
            
            # ËÆ°ÁÆóË£ÖÈ•∞Èü≥Êï∞ÈáèÔºà‰º∞ËÆ°ÂÄºÔºâ
            analysis['ornament_notes'] = output_analysis.get('ornament_tokens', 0)
            
            # Ë£ÖÈ•∞Èü≥ÂØÜÂ∫¶
            if pitch_tokens > 0:
                analysis['ornament_density'] = analysis['ornament_notes'] / pitch_tokens
            
            # Ë£ÖÈ•∞Èü≥Á±ªÂûãÁªüËÆ°
            analysis['ornament_types'] = output_analysis.get('ornament_categories', {
                'Áü≠Èü≥Á¨¶': 0,
                'È´òÂäõÂ∫¶': 0,
                'ÂæÆÊó∂Â∫èË∞ÉÊï¥': 0,
                'Ë£ÖÈ•∞ÊÄßÈü≥È´ò': 0
            })
            
            # ÂæÆÊó∂Â∫èË∞ÉÊï¥
            analysis['microtiming_adjustments'] = analysis['ornament_types'].get('ÂæÆÊó∂Â∫èË∞ÉÊï¥', 0)
            
            return analysis
            
        except Exception as e:
            print(f"Ë£ÖÈ•∞Èü≥ÂàÜÊûêÂ§±Ë¥•: {e}")
            return {}
            
    def midi_to_score(self, midi_path, output_path, highlight_ornaments=False, reference_midi=None):
        """Convert MIDI file to MusicXML format for OpenSheetMusicDisplay
        
        Args:
            midi_path: Path to MIDI file
            output_path: Path for output MusicXML file (should end with .xml)
            highlight_ornaments: Whether to highlight ornaments with colors
            reference_midi: Reference MIDI file path (for comparing to identify ornaments)
            
        Returns:
            bool: Whether conversion was successful
        """
        try:
            import music21
            from music21 import stream, note, pitch, duration, meter, tempo, key, clef
            
            # Load MIDI file using music21
            score = music21.converter.parse(midi_path)
            
            # Get reference notes if highlighting ornaments
            ref_notes = set()
            if highlight_ornaments and reference_midi:
                try:
                    ref_score = music21.converter.parse(reference_midi)
                    for part in ref_score.parts:
                        for element in part.flat.notes:
                            if hasattr(element, 'pitch'):
                                # Use pitch and quantized offset as key
                                offset = round(element.offset * 4) / 4  # Quantize to 16th notes
                                ref_notes.add((element.pitch.midi, offset))
                            elif hasattr(element, 'pitches'):  # Chord
                                offset = round(element.offset * 4) / 4
                                for p in element.pitches:
                                    ref_notes.add((p.midi, offset))
                except Exception as e:
                    print(f"Failed to load reference MIDI: {e}")
            
            # Create a new score with proper formatting
            new_score = stream.Score()
            
            # Add metadata
            new_score.metadata = music21.metadata.Metadata()
            new_score.metadata.title = 'Ornament Generation Result'
            new_score.metadata.composer = 'AI Generated'
            
            # Create a single part to merge all voices
            new_part = stream.Part()
            new_part.partName = 'Piano'
            new_part.partAbbreviation = 'Pno'
            
            # Add clef, key signature, time signature, and tempo
            new_part.insert(0, clef.TrebleClef())
            new_part.insert(0, key.KeySignature(0))  # C major
            new_part.insert(0, meter.TimeSignature('4/4'))
            new_part.insert(0, tempo.TempoIndication(number=120))
            
            # Collect all notes from all parts by offset
            # (ÊóßÂÆûÁé∞ÊåâËµ∑Âßã offset ÂêàÂπ∂ÔºåÂèØËÉΩÂØºËá¥Ë∑®Êó∂ÂÄºÈáçÂè†Ôºå‰ªéËÄåÂú®ÂØºÂá∫Êó∂‰∫ßÁîüÈöêÂºèÂ§öÂ£∞ÈÉ®Ôºå‰ºëÊ≠¢Á¨¶‰ºöË¢´ OSMD/engraver Êé®Âà∞‰∫îÁ∫øË∞±Â§ñ‰æß)
            # Êîπ‰∏∫ÔºöÂü∫‰∫é‰∫ã‰ª∂Êó∂Èó¥ÁâáÔºàtime-slicingÔºâÁöÑÊñπÂºèÊûÑÂª∫ÂçïÂ£∞ÈÉ®Ôºö
            # 1) Êî∂ÈõÜÊâÄÊúâÈü≥ÁöÑËµ∑Ê≠¢Êó∂Èó¥
            # 2) ÁîüÊàêÂÖ®Â±ÄÊñ≠ÁÇπÂ∫èÂàóÔºàËµ∑ÁÇπ‰∏éÁªàÁÇπÔºâÂπ∂ÈáèÂåñ
            # 3) Âú®ÊØè‰∏™Áõ∏ÈÇªÊñ≠ÁÇπÂå∫Èó¥ÂÜÖÔºåÂÜôÂÖ•ÂΩìÂâç‚ÄúÊ≠£Âú®ÂèëÂ£∞‚ÄùÁöÑÈü≥ÈõÜÂêàÔºàÊó†ÂàôÂÜô‰ºëÊ≠¢Á¨¶Ôºâ
            
            # Êî∂ÈõÜÊâÄÊúâ note ‰∫ã‰ª∂ÔºàÂåÖÂê´ÂçïÈü≥ÂíåÂíåÂº¶ÁöÑÊØè‰∏™Èü≥Ôºâ
            events = []  # (pitch_obj, start, end, velocity)
            for part_idx, part in enumerate(score.parts):
                for element in part.flat.notes:
                    vel = getattr(element.volume, 'velocity', 64)
                    if hasattr(element, 'pitch'):
                        events.append((element.pitch, float(element.offset), float(element.offset + element.quarterLength), vel))
                    elif hasattr(element, 'pitches'):
                        for p in element.pitches:
                            events.append((p, float(element.offset), float(element.offset + element.quarterLength), vel))
            
            if not events:
                # Ê≤°ÊúâÈü≥Á¨¶ÂàôÁõ¥Êé•ÂÜô‰∏Ä‰∏™ÂÖ®‰ºëÊ≠¢ÁöÑÂ∞èËäÇÔºåÈÅøÂÖçÂêéÁª≠Êä•Èîô
                r = note.Rest(quarterLength=4.0)
                try:
                    # ËÆ©‰ºëÊ≠¢Á¨¶Â±Ö‰∏≠ÊòæÁ§∫Ôºàtreble ‰∏≠ÂøÉÁ∫ø B4Ôºâ
                    r.staffPosition = 0
                except Exception:
                    pass
                new_part.insert(0.0, r)
            else:
                # ÁîüÊàêÊñ≠ÁÇπÔºàÊâÄÊúâÂºÄÂßã‰∏éÁªìÊùüÔºâÔºåÂπ∂ËøõË°åËΩªÈáèÈáèÂåñ‰ª•ÈÅøÂÖçÊµÆÁÇπÊäñÂä®
                quantize_div = 16  # 1/16Èü≥Á¨¶Á≤íÂ∫¶
                def q(x: float) -> float:
                    return round(x * quantize_div) / quantize_div
                
                breakpoints = set()
                for _, s, e, _ in events:
                    breakpoints.add(q(s))
                    breakpoints.add(q(e))
                # Á°Æ‰øùÂåÖÂê´ 0 Ëµ∑ÁÇπ
                breakpoints.add(0.0)
                points = sorted([p for p in breakpoints])
                
                # ‰∏∫Âø´ÈÄüÊü•ËØ¢ÔºåÊåâËµ∑ÁÇπÊéíÂ∫è
                events.sort(key=lambda it: it[1])
                
                # ÈÄêÂå∫Èó¥ÂÜôÂÖ•ÂÜÖÂÆπ
                for i in range(len(points) - 1):
                    start = points[i]
                    end = points[i + 1]
                    if end <= start:
                        continue
                    duration = end - start
                    
                    # ÊâæÂà∞Âú®ËØ•Âå∫Èó¥ÂÜÖÂ§Ñ‰∫éÂèëÂ£∞Áä∂ÊÄÅÁöÑÈü≥Ôºàstart ‚àà [s, e)Ôºâ
                    active_pitches = []
                    avg_velocity = 0
                    cnt = 0
                    for p_obj, s, e, v in events:
                        # ÂÖÅËÆ∏ÊûÅÂ∞èÁöÑÊµÆÁÇπËØØÂ∑Æ
                        if s - 1e-6 <= start < e - 1e-6:
                            active_pitches.append(p_obj)
                            avg_velocity += v
                            cnt += 1
                    if cnt > 0:
                        avg_velocity = int(avg_velocity / cnt)
                    else:
                        avg_velocity = 64
                    
                    if len(active_pitches) == 0:
                        # Á©∫Âå∫Èó¥ -> ÂÜôÂÖ•‰ºëÊ≠¢Á¨¶ÔºàÊòæÂºèÔºâÔºåÈÅøÂÖçËá™Âä®Ë°•ÈΩê‰∫ßÁîüÁöÑÂ§öÂ£∞ÈÉ®‰∏éÊºÇÁßª
                        r = note.Rest(quarterLength=duration)
                        try:
                            r.staffPosition = 0  # Â∞ΩÈáèÂ±Ö‰∏≠
                        except Exception:
                            pass
                        r.offset = start
                        new_part.insert(start, r)
                    elif len(active_pitches) == 1:
                        # ÂçïÈü≥
                        p = active_pitches[0]
                        new_element = note.Note(p, quarterLength=duration)
                        new_element.offset = start
                        new_element.volume.velocity = avg_velocity
                        
                        if highlight_ornaments:
                            # ‰ΩøÁî®Êõ¥ÂÆΩÊùæÁöÑÂåπÈÖçÔºöÊ£ÄÊü•ËØ•Èü≥È´òÊòØÂê¶Âú®ÂèÇËÄÉÈü≥Á¨¶‰∏≠Â≠òÂú®ÔºàÂøΩÁï•Á≤æÁ°ÆÊó∂Èó¥ÂåπÈÖçÔºâ
                            is_ornament = not any(ref_pitch == p.midi for ref_pitch, _ in ref_notes)
                            new_element.style.color = '#000000'  # ÊâÄÊúâÈü≥Á¨¶ÈÉΩ‰ΩøÁî®ÈªëËâ≤
                            new_element.addLyric(f'{p.name}{p.octave}')
                        else:
                            new_element.addLyric(f'{p.name}{p.octave}')
                        new_part.insert(start, new_element)
                    else:
                        # ÂíåÂº¶ÔºàÂ§ö‰∏™Èü≥ÂêåÊó∂Âú®ËØ•Âå∫Èó¥ÂèëÂ£∞Ôºâ
                        # ÂéªÈáç‰ª•ÂÖçÁõ∏ÂêåÈü≥ÈáçÂ§ç
                        unique_pitches = []
                        seen = set()
                        for p in active_pitches:
                            if p.midi not in seen:
                                seen.add(p.midi)
                                unique_pitches.append(p)
                        new_element = music21.chord.Chord(unique_pitches, quarterLength=duration)
                        new_element.offset = start
                        new_element.volume.velocity = avg_velocity
                        
                        if highlight_ornaments:
                            # ‰ΩøÁî®Êõ¥ÂÆΩÊùæÁöÑÂåπÈÖçÔºöËã•ÂíåÂº¶‰∏≠‰ªª‰∏ÄÈü≥È´òÂú®ÂèÇËÄÉÈü≥Á¨¶‰∏≠Â≠òÂú®ÔºåÂàôËßÜ‰∏∫ÈùûË£ÖÈ•∞Èü≥
                            has_ref = any(any(ref_pitch == p.midi for ref_pitch, _ in ref_notes) for p in unique_pitches)
                            new_element.style.color = '#000000'  # ÊâÄÊúâÈü≥Á¨¶ÈÉΩ‰ΩøÁî®ÈªëËâ≤
                        new_part.insert(start, new_element)
            
            new_score.insert(0, new_part)
            
            # Ensure output directory exists
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # Write MusicXML file
            new_score.write('musicxml', fp=output_path)
            
            print(f"Successfully converted MIDI to MusicXML: {output_path}")
            return True
            
        except Exception as e:
            print(f"Error converting MIDI to MusicXML: {e}")
            # Create a fallback empty MusicXML
            try:
                fallback_xml = '''<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE score-partwise PUBLIC "-//Recordare//DTD MusicXML 3.1 Partwise//EN" "http://www.musicxml.org/dtds/partwise.dtd">
<score-partwise version="3.1">
  <movement-title>Error Loading MIDI</movement-title>
  <part-list>
    <score-part id="P1">
      <part-name>Error</part-name>
    </score-part>
  </part-list>
  <part id="P1">
    <measure number="1">
      <attributes>
        <divisions>1</divisions>
        <time><beats>4</beats><beat-type>4</beat-type></time>
        <clef><sign>G</sign><line>2</line></clef>
      </attributes>
      <note><rest/><duration>4</duration><type>whole</type></note>
    </measure>
  </part>
</score-partwise>'''
                
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(fallback_xml)
                return False
            except Exception as fallback_error:
                print(f"Failed to create fallback MusicXML: {fallback_error}")
                return False

    def _last_non_control(self, seq):
        for t in reversed(seq):
            if t not in self._ids_timeshift and t not in self._ids_micro:
                return t
        return None

    def _apply_syntax_biases(self, seq, logits):
        """ÂØπ‰∏ã‰∏ÄÊ≠• logits ÊñΩÂä†ÁÆÄÂçïËØ≠Ê≥ïÂÅèÁΩÆÔºåÂáèÂ∞ë‰ºëÊ≠¢/Á©∫ÁôΩÂπ∂ÊèêÂçáPitch/DurationÊ¶ÇÁéá„ÄÇ"""
        with torch.no_grad():
            bias = torch.zeros_like(logits)
            last_tok = self._last_non_control(seq)

            # ÈÄöÁî®ÔºöÈôç‰ΩéËøûÁª≠ TimeShift/MicroTiming Ê¶ÇÁéá
            if len(seq) >= 1 and (seq[-1] in self._ids_timeshift or seq[-1] in self._ids_micro):
                bias[list(self._ids_timeshift)] -= 0.4
                bias[list(self._ids_micro)] -= 0.2

            # Ëã•‰∏ä‰∏Ä‰∏™ÈùûÊéßÂà∂‰∏çÊòØ PitchÔºåÂàôÊõ¥Â∏åÊúõ‰∏ã‰∏ÄÊ≠•ÊòØ Pitch
            if last_tok is None or last_tok not in self._ids_pitch:
                bias[list(self._ids_pitch)] += 0.9
                # ÂêåÊó∂ÊäëÂà∂ÁªßÁª≠TimeShift
                bias[list(self._ids_timeshift)] -= 0.3
                # ÈºìÂä±Áü≠TimeShift‰ª•ÂáèÂ∞ëÈïøÂÅúÈ°ø
                if hasattr(self, '_ids_timeshift_short'):
                    bias[list(self._ids_timeshift_short)] += 0.2
            else:
                # ‰∏ä‰∏Ä‰∏™ÊòØ PitchÔºö‰∏ã‰∏ÄÊ≠•ÈºìÂä± Velocity Êàñ DurationÔºà‰ºòÂÖàÁªôÂá∫Êó∂ÂÄº/ÂäõÂ∫¶Ôºâ
                bias[list(self._ids_velocity)] += 0.5
                bias[list(self._ids_duration)] += 0.8

            # ËΩØÁ∫¶ÊùüÔºöÊéßÂà∂Á±ªtokenÊï¥‰ΩìËΩªÂ∫¶ÈôçÊùÉ
            bias[list(self._ids_micro)] -= 0.2

            # ÂÖ®Â±ÄÔºöÊÉ©ÁΩöÈïøTimeShiftÔºåÈºìÂä±Áü≠TimeShift
            if hasattr(self, '_ids_timeshift_long'):
                bias[list(self._ids_timeshift_long)] -= 0.6
            if hasattr(self, '_ids_timeshift_short'):
                bias[list(self._ids_timeshift_short)] += 0.4

            return logits + bias

    def _sanitize_tokens(self, tokens):
        """Ê∏ÖÊ¥óÁîüÊàêÂ∫èÂàó‰ª•Êª°Ë∂≥ PerTok ËØ≠Ê≥ïÔºö
        - Á°Æ‰øùÂºÄÂ§¥Êúâ TimeSig Êàñ BOSÔºàËã•Â≠òÂú®Ôºâ
        - ÂêàÂπ∂ËøûÁª≠ TimeShift/MicroTimingÔºà‰ªÖ‰øùÁïô‰∏Ä‰∏™Ôºâ
        - ‰∏∫Êó† Duration ÁöÑ Pitch Ë°•ÈªòËÆ§ Duration
        - ÂéªÈô§ÁªìÂ∞æÂ§ö‰ΩôÁöÑÊéßÂà∂Á±ª token
        """
        vocab = self.tokenizer.vocab
        id_to_str = {v: k for k, v in vocab.items()}

        def default_duration_id():
            # ÈÄâÊã©‰∏Ä‰∏™Â∏∏ËßÅÊó∂ÂÄºÔºà0.80.. Á∫¶ÂçäÊãç/ÊãçÔºâÔºåÂõûÈÄÄÂà∞Á¨¨‰∏Ä‰∏™Duration
            for tid in self._ids_duration:
                s = id_to_str.get(tid, '')
                if s.startswith('Duration_0.8') or s.startswith('Duration_1'):
                    return tid
            return next(iter(self._ids_duration)) if self._ids_duration else None

        # ‰ªÖ‰øùÁïôÂÖÅËÆ∏ÁöÑtokenÔºåÈÅøÂÖçPerTokÊó†Ê≥ïËß£ÊûêÁöÑ‚Äúother‚ÄùÁ±ª
        filtered = [t for t in tokens if t in self._ids_allowed]

        out = []
        # Ëã•Êúâ TimeSigÔºåÁ°Æ‰øùÊîæÂú®ÊúÄÂâç
        seen_content = False
        for t in filtered:
            if not seen_content and t in self._ids_timesig:
                out.append(t)
                continue
            if t not in self._ids_timeshift and t not in self._ids_micro and t not in self._ids_special:
                seen_content = True
            out.append(t)

        # Ëã•ÂºÄÂ§¥Ê≤°ÊúâTimeSig‰∏îËØçË°®Êúâ4/4ÔºåÊèíÂÖ•‰∏ÄÊûö
        if (not out) or (out[0] not in self._ids_timesig):
            if self._id_timesig_44 is not None:
                out.insert(0, self._id_timesig_44)

        # ÂêàÂπ∂ËøûÁª≠ TimeShift/MicroTiming
        merged = []
        prev_is_ts = False
        prev_is_micro = False
        for t in out:
            if t in self._ids_timeshift:
                if prev_is_ts:
                    continue
                prev_is_ts = True
                prev_is_micro = False
            elif t in self._ids_micro:
                if prev_is_micro:
                    continue
                prev_is_micro = True
                prev_is_ts = False
            else:
                prev_is_ts = prev_is_micro = False
            merged.append(t)

        # ‰∫ã‰ª∂Á∫ßÈáçÊéíÔºöÁ°Æ‰øù [Pitch][Velocity?][MicroTiming?][Duration] ÁöÑÈ°∫Â∫è
        cleaned = []
        i = 0
        dur_id = default_duration_id()
        while i < len(merged):
            t = merged[i]
            if t in self._ids_pitch:
                pitch_tok = t
                vel_tok = None
                micro_tok = None
                dur_tok = None
                look_end = min(i + 8, len(merged))
                k = i + 1
                while k < look_end:
                    tk = merged[k]
                    if tk in self._ids_velocity and vel_tok is None:
                        vel_tok = tk
                    elif tk in self._ids_micro and micro_tok is None:
                        micro_tok = tk
                    elif tk in self._ids_duration and dur_tok is None:
                        dur_tok = tk
                    elif tk in self._ids_pitch or tk in self._ids_timeshift:
                        break
                    k += 1
                cleaned.append(pitch_tok)
                if vel_tok is not None:
                    cleaned.append(vel_tok)
                if micro_tok is not None:
                    cleaned.append(micro_tok)
                cleaned.append(dur_tok if dur_tok is not None else dur_id)
                # Ë∑≥ËøáÂ∑≤Ê∂àË¥πÁöÑÁ™óÂè£
                i = k
                continue
            else:
                cleaned.append(t)
                i += 1

        # ÂéªÈô§ÁªìÂ∞æÂ§ö‰ΩôÊéßÂà∂ token
        while cleaned and (cleaned[-1] in self._ids_timeshift or cleaned[-1] in self._ids_micro):
            cleaned.pop()

        # Ëã•Êúâ BOS/EOS ÂàôË°•ÈΩêÔºàÊúÄÂêéÂÜçË°•Ôºâ
        bos = vocab.get('BOS_None')
        eos = vocab.get('EOS_None')
        if bos is not None and (len(cleaned) == 0 or cleaned[0] != bos):
            cleaned.insert(0, bos)
        if eos is not None and cleaned[-1] != eos:
            cleaned.append(eos)

        return cleaned
    def decode_to_midi(self, tokens, output_path):
        """Â∞ÜtokenÂ∫èÂàóËß£Á†Å‰∏∫MIDIÊñá‰ª∂Âπ∂‰øùÂ≠ò
        
        Args:
            tokens: tokenÂ∫èÂàó
            output_path: ËæìÂá∫MIDIÊñá‰ª∂Ë∑ØÂæÑ
            
        Returns:
            bool: Ëß£Á†ÅÊòØÂê¶ÊàêÂäü
        """
        try:
            print(f"üéº Ëß£Á†ÅtokenÂ∫èÂàó‰∏∫MIDI: {len(tokens)}‰∏™tokens")
            
            # ‰ΩøÁî®FixedPerTokDecoderËß£Á†Å
            score = self.decoder.decode_tokens(tokens)
            if score is None:
                print("‚ùå Ëß£Á†ÅÂ§±Ë¥•")
                return False
                
            # Ëé∑ÂèñÈü≥Á¨¶Êï∞Èáè
            total_notes = sum(len(t.notes) for t in getattr(score, 'tracks', []))
            print(f"  ‚úÖ PerTokÊû∂ÊûÑËß£Á†ÅÂÆåÊàê: {total_notes}‰∏™Èü≥Á¨¶")
            
            # ‰øùÂ≠òMIDIÊñá‰ª∂
            success = self.decoder.save_to_midi(score, output_path)
            if success:
                file_size = os.path.getsize(output_path)
                print(f"‚úÖ MIDI‰øùÂ≠òÊàêÂäü: ")
                print(f"   Êñá‰ª∂: {output_path} ({file_size} bytes)")
                print(f"   ËΩ®ÈÅìÊï∞: {len(getattr(score, 'tracks', []))}")
                print(f"   Èü≥Á¨¶Êï∞: {total_notes}")
                return True
            else:
                print("‚ùå MIDI‰øùÂ≠òÂ§±Ë¥•")
                return False
                
        except Exception as e:
            print(f"‚ùå MIDIËß£Á†ÅÂ§±Ë¥•: {e}")
            return False
            
    def analyze_ornament_content(self, tokens):
        """ÂàÜÊûêtokenÂ∫èÂàó‰∏≠ÁöÑË£ÖÈ•∞Èü≥ÂÜÖÂÆπ"""
        try:
            analyzer = self.ornament_loss.analyzer
            
            analysis = {
                'total_tokens': len(tokens),
                'ornament_tokens': 0,
                'ornament_ratio': 0.0,
                'ornament_categories': {},
                'average_weight': 0.0
            }
            
            weights = []
            for token_id in tokens:
                if token_id < len(self.tokenizer.vocab):
                    weight = analyzer.get_ornament_weight(token_id)
                    weights.append(weight)
                    
                    if analyzer.is_ornament_token(token_id):
                        analysis['ornament_tokens'] += 1
                        
                        # ÁªüËÆ°Ë£ÖÈ•∞Èü≥Á±ªÂà´
                        for category, token_set in analyzer.ornament_tokens.items():
                            if token_id in token_set:
                                analysis['ornament_categories'][category] = analysis['ornament_categories'].get(category, 0) + 1
            
            if analysis['total_tokens'] > 0:
                analysis['ornament_ratio'] = analysis['ornament_tokens'] / analysis['total_tokens']
                analysis['average_weight'] = sum(weights) / len(weights) if weights else 1.0
            
            return analysis
            
        except Exception as e:
            print(f"‚ùå Ë£ÖÈ•∞Èü≥ÂàÜÊûêÂ§±Ë¥•: {e}")
            return {}
    
    def generate_from_midi(self, input_path: str, output_path: str, **kwargs):
        """ÂÆåÊï¥ÁöÑMIDIÂà∞MIDIË£ÖÈ•∞Èü≥ÁîüÊàêÊµÅÁ®ã"""
        print("üöÄ ÂºÄÂßãÂÆåÊï¥Ë£ÖÈ•∞Èü≥ÁîüÊàêÊµÅÁ®ã")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. ÁºñÁ†ÅËæìÂÖ•MIDI
        print("\n1Ô∏è‚É£ ÁºñÁ†ÅËæìÂÖ•MIDI")
        input_tokens = self.encode_midi(input_path)
        if input_tokens is None:
            print("‚ùå ÊµÅÁ®ãÁªàÊ≠¢ÔºöMIDIÁºñÁ†ÅÂ§±Ë¥•")
            return False
        
        # 2. ÂàÜÊûêËæìÂÖ•ÂÜÖÂÆπ
        print("\n2Ô∏è‚É£ ÂàÜÊûêËæìÂÖ•ÂÜÖÂÆπ")
        input_analysis = self.analyze_ornament_content(input_tokens)
        print(f"   ÊÄªtokens: {input_analysis['total_tokens']}")
        print(f"   Ë£ÖÈ•∞Èü≥tokens: {input_analysis['ornament_tokens']} ({input_analysis['ornament_ratio']:.1%})")
        print(f"   Âπ≥ÂùáÊùÉÈáç: {input_analysis['average_weight']:.2f}")
        
        # 3. ÁîüÊàêË£ÖÈ•∞Èü≥
        print("\n3Ô∏è‚É£ ÁîüÊàêË£ÖÈ•∞Èü≥")
        generated_tokens = self.generate_ornaments(input_tokens, **kwargs)
        if generated_tokens is None:
            print("‚ùå ÊµÅÁ®ãÁªàÊ≠¢ÔºöË£ÖÈ•∞Èü≥ÁîüÊàêÂ§±Ë¥•")
            return False
        
        # 4. ÂàÜÊûêÁîüÊàêÂÜÖÂÆπ
        print("\n4Ô∏è‚É£ ÂàÜÊûêÁîüÊàêÂÜÖÂÆπ")
        output_analysis = self.analyze_ornament_content(generated_tokens)
        print(f"   ÊÄªtokens: {output_analysis['total_tokens']}")
        print(f"   Ë£ÖÈ•∞Èü≥tokens: {output_analysis['ornament_tokens']} ({output_analysis['ornament_ratio']:.1%})")
        print(f"   Âπ≥ÂùáÊùÉÈáç: {output_analysis['average_weight']:.2f}")
        
        # 5. Ëß£Á†Å‰∏∫MIDI
        print("\n5Ô∏è‚É£ Ëß£Á†Å‰∏∫MIDI")
        decode_success = self.decode_to_midi(generated_tokens, output_path)
        if not decode_success:
            print("‚ùå ÊµÅÁ®ãÁªàÊ≠¢ÔºöMIDIËß£Á†ÅÂ§±Ë¥•")
            return False
        
        # 6. ÊÄªÁªì
        elapsed_time = time.time() - start_time
        ornament_enhancement = output_analysis['ornament_ratio'] - input_analysis['ornament_ratio']
        
        print("\n" + "=" * 60)
        print("üìä ÊµÅÁ®ãÂÆåÊàêÊÄªÁªì")
        print("=" * 60)
        print(f"‚úÖ ËæìÂÖ•Êñá‰ª∂: {input_path}")
        print(f"‚úÖ ËæìÂá∫Êñá‰ª∂: {output_path}")
        print(f"‚è±Ô∏è  ÊÄªËÄóÊó∂: {elapsed_time:.1f}Áßí")
        print(f"üéµ ËæìÂÖ•Ë£ÖÈ•∞Èü≥: {input_analysis['ornament_ratio']:.1%}")
        print(f"üé® ËæìÂá∫Ë£ÖÈ•∞Èü≥: {output_analysis['ornament_ratio']:.1%}")
        print(f"üìà Ë£ÖÈ•∞Èü≥Â¢ûÂº∫: {ornament_enhancement:+.1%}")
        
        if ornament_enhancement > 0:
            print("\nüéâ Ë£ÖÈ•∞Èü≥ÁîüÊàêÊàêÂäüÂÆåÊàêÔºÅ")
        else:
            print("\n‚ö†Ô∏è  Ê≥®ÊÑèÔºöÁîüÊàêÁöÑË£ÖÈ•∞Èü≥ÊØî‰æãÊú™Â¢ûÂä†ÔºåÂèØËÉΩÈúÄË¶ÅË∞ÉÊï¥ÁîüÊàêÂèÇÊï∞")
        
        return True


def main():
    parser = argparse.ArgumentParser(
        description='Ë£ÖÈ•∞Èü≥ÁîüÊàêÊé®ÁêÜ - Leveraging PerTok and Domain-Specific Transformer Design for Expressive MIDI Ornament Generation',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Á§∫‰æãÁî®Ê≥ï:
  # Âü∫Á°ÄÁîüÊàê
  python inference.py --input examples/bach.mid --output output/bach_ornament.mid
  
  # Ë∞ÉÊï¥ÁîüÊàêÂèÇÊï∞
  python inference.py --input input.mid --output output.mid --temperature 1.2 --top_k 50 --top_p 0.9
  
  # ‰ΩøÁî®ÁâπÂÆöÊ®°Âûã
  python inference.py --model checkpoints_ornament_aware/best_ornament_aware_model.pth --input input.mid --output output.mid
        """)
    
    # ÂøÖÈúÄÂèÇÊï∞
    parser.add_argument('--input', '-i', type=str, required=True,
                        help='ËæìÂÖ•MIDIÊñá‰ª∂Ë∑ØÂæÑ')
    parser.add_argument('--output', '-o', type=str, required=True,
                        help='ËæìÂá∫MIDIÊñá‰ª∂Ë∑ØÂæÑ')
    
    # Ê®°ÂûãÂèÇÊï∞
    parser.add_argument('--model', '-m', type=str, 
                        default='checkpoints_ornament_aware/best_ornament_aware_model.pth',
                        help='Ê®°ÂûãÊñá‰ª∂Ë∑ØÂæÑ (ÈªòËÆ§: ÊúÄÊñ∞ÁöÑOrnamentAwareÊ®°Âûã)')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'cuda', 'cpu'],
                        help='ËÆ°ÁÆóËÆæÂ§á')
    parser.add_argument('--allow_fallback', action='store_true', default=False,
                        help='ÂÖÅËÆ∏Âú®PerTokËß£Á†ÅÂ§±Ë¥•Êó∂ÂõûÈÄÄÂà∞ÊâãÂÜôËß£Á†ÅÔºàÈªòËÆ§‰∏•Ê†ºÊ®°ÂºèÔºö‰∏çÂõûÈÄÄÔºâ')
    
    # ÁîüÊàêÂèÇÊï∞
    parser.add_argument('--temperature', '-t', type=float, default=1.1,
                        help='ÁîüÊàêÊ∏©Â∫¶ (ÈªòËÆ§: 1.1, ËåÉÂõ¥: 0.5-2.0)')
    parser.add_argument('--top_k', type=int, default=50,
                        help='Top-kÈááÊ†∑ (ÈªòËÆ§: 50)')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='Top-pÈááÊ†∑ (ÈªòËÆ§: 0.9)')
    
    # ËæìÂá∫ÊéßÂà∂
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='ËØ¶ÁªÜËæìÂá∫')
    parser.add_argument('--force', '-f', action='store_true',
                        help='Âº∫Âà∂Ë¶ÜÁõñËæìÂá∫Êñá‰ª∂')
    
    args = parser.parse_args()
    
    # Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂
    if not os.path.exists(args.input):
        print(f"‚ùå ËæìÂÖ•Êñá‰ª∂‰∏çÂ≠òÂú®: {args.input}")
        return 1
    
    # Ê£ÄÊü•ËæìÂá∫ÁõÆÂΩï
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"üìÅ ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï: {output_dir}")
    
    # Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
    if os.path.exists(args.output) and not args.force:
        response = input(f"‚ö†Ô∏è  ËæìÂá∫Êñá‰ª∂Â∑≤Â≠òÂú®: {args.output}\nÊòØÂê¶Ë¶ÜÁõñ? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("‚ùå Êìç‰ΩúÂèñÊ∂à")
            return 1
    
    # Ê£ÄÊü•Ê®°ÂûãÊñá‰ª∂
    if not os.path.exists(args.model):
        print(f"‚ùå Ê®°ÂûãÊñá‰ª∂‰∏çÂ≠òÂú®: {args.model}")
        print("üí° ËØ∑Á°Æ‰øùÂ∑≤ËÆ≠ÁªÉÊ®°ÂûãÊàñÊåáÂÆöÊ≠£Á°ÆÁöÑÊ®°ÂûãË∑ØÂæÑ")
        return 1
    
    # ÂèÇÊï∞È™åËØÅ
    if not (0.1 <= args.temperature <= 3.0):
        print(f"‚ùå Ê∏©Â∫¶ÂèÇÊï∞Ë∂ÖÂá∫ËåÉÂõ¥: {args.temperature} (Â∫îÂú®0.1-3.0‰πãÈó¥)")
        return 1
    
    if not (1 <= args.top_k <= 200):
        print(f"‚ùå top_kÂèÇÊï∞Ë∂ÖÂá∫ËåÉÂõ¥: {args.top_k} (Â∫îÂú®1-200‰πãÈó¥)")
        return 1
    
    if not (0.1 <= args.top_p <= 1.0):
        print(f"‚ùå top_pÂèÇÊï∞Ë∂ÖÂá∫ËåÉÂõ¥: {args.top_p} (Â∫îÂú®0.1-1.0‰πãÈó¥)")
        return 1
    
    try:
        # ÂàùÂßãÂåñÊé®ÁêÜÂºïÊìé
        engine = OrnamentInferenceEngine(args.model, args.device, allow_fallback=args.allow_fallback)
        
        # ÁîüÊàêË£ÖÈ•∞Èü≥
        success = engine.generate_from_midi(
            input_path=args.input,
            output_path=args.output,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\n‚ùå Áî®Êà∑‰∏≠Êñ≠")
        return 1
    except Exception as e:
        print(f"‚ùå ÂèëÁîüÈîôËØØ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
