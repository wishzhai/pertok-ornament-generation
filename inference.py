#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„è£…é¥°éŸ³ç”Ÿæˆæ¨ç†è„šæœ¬
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼Œä¸è®ºæ–‡æµç¨‹å›¾å®Œå…¨ä¸€è‡´
"""

import torch
import argparse
import os
import time
from pathlib import Path

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from ornament_model import OrnamentTransformer
from working_pertok_config import create_working_config, create_working_tokenizer
from fixed_pertok_decoder import FixedPerTokDecoder
from ornament_aware_loss import OrnamentAwareLoss, create_ornament_aware_loss


class OrnamentInferenceEngine:
    """è£…é¥°éŸ³ç”Ÿæˆæ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, device: str = "auto", allow_fallback: bool = False):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ("auto", "cuda", "cpu")
        """
        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ åˆå§‹åŒ–è£…é¥°éŸ³ç”Ÿæˆå¼•æ“")
        print(f"   è®¾å¤‡: {self.device}")
        self.allow_fallback = allow_fallback
        
        # åˆå§‹åŒ–tokenizerå’Œè§£ç å™¨
        self.tokenizer = create_working_tokenizer()
        self.decoder = FixedPerTokDecoder(self.tokenizer)
        
        # åˆå§‹åŒ–è£…é¥°éŸ³æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆç”¨äºåˆ†æï¼‰
        self.ornament_loss = create_ornament_aware_loss(self.tokenizer)
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        
        if self.model is None:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_path}")
        
        print(f"âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")

        # é¢„è®¡ç®—tokenç±»å‹é›†åˆï¼ˆç”¨äºè¯­æ³•çº¦æŸä¸sanitizerï¼‰
        self._build_token_sets()
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        try:
            print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # é‡å»ºæ¨¡å‹æ¶æ„
            vocab_size = checkpoint['vocab_size']
            model = OrnamentTransformer(
                vocab_size=vocab_size,
                max_seq_len=checkpoint.get('max_seq_len', 512),
                d_model=checkpoint.get('d_model', 512),
                n_heads=checkpoint.get('n_heads', 8),
                n_layers=checkpoint.get('n_layers', 8)
            )
            
            # åŠ è½½æƒé‡
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(self.device)
            model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ:")
            print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            print(f"   æ¨¡å‹ç»´åº¦: {checkpoint.get('d_model', 512)}")
            print(f"   è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'æœªçŸ¥')}")
            print(f"   éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'æœªçŸ¥')}")
            
            return model
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def encode_midi(self, midi_path: str):
        """ç¼–ç MIDIæ–‡ä»¶ä¸ºtokenåºåˆ—"""
        try:
            print(f"ğŸµ ç¼–ç MIDIæ–‡ä»¶: {midi_path}")
            
            if not os.path.exists(midi_path):
                raise FileNotFoundError(f"MIDIæ–‡ä»¶ä¸å­˜åœ¨: {midi_path}")
            
            tokenized_result = self.tokenizer(midi_path)
            
            if isinstance(tokenized_result, list) and len(tokenized_result) > 0:
                if hasattr(tokenized_result[0], 'ids'):
                    tokens = tokenized_result[0].ids
                    print(f"   ç¼–ç æˆåŠŸ: {len(tokens)} ä¸ªtokens")
                    return tokens
            
            raise ValueError("ç¼–ç ç»“æœæ— æ•ˆ")
            
        except Exception as e:
            print(f"âŒ MIDIç¼–ç å¤±è´¥: {e}")
            return None
    
    def generate_ornaments(self, input_tokens, temperature=1.0, top_k=50, top_p=0.9, max_new_tokens=None):
        """è‡ªå›å½’ç”Ÿæˆè£…é¥°éŸ³åºåˆ—"""
        if self.model is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½")
            return None
        
        try:
            print(f"ğŸ¨ è‡ªå›å½’ç”Ÿæˆè£…é¥°éŸ³...")
            print(f"   è¾“å…¥é•¿åº¦: {len(input_tokens)}")
            print(f"   å‚æ•°: temperature={temperature}, top_k={top_k}, top_p={top_p}")
            
            # è®¾ç½®ç”Ÿæˆé•¿åº¦
            if max_new_tokens is None:
                max_new_tokens = min(len(input_tokens) * 2, self.model.max_seq_len - len(input_tokens))
            
            # åˆå§‹åŒ–ç”Ÿæˆåºåˆ—ä¸ºè¾“å…¥tokens
            generated_sequence = input_tokens.copy()
            
            # ç‰¹æ®Štokens
            eos_token = 2  # å‡è®¾EOS token IDä¸º2
            pad_token = 0  # å‡è®¾PAD token IDä¸º0
            
            with torch.no_grad():
                for step in range(max_new_tokens):
                    # å½“å‰åºåˆ—è½¬ä¸ºtensor
                    current_len = len(generated_sequence)
                    if current_len >= self.model.max_seq_len:
                        break
                    
                    # æˆªæ–­åºåˆ—åˆ°æ¨¡å‹æœ€å¤§é•¿åº¦
                    input_sequence = generated_sequence[-self.model.max_seq_len:]
                    input_tensor = torch.tensor([input_sequence], dtype=torch.long, device=self.device)
                    
                    # æ¨¡å‹å‰å‘æ¨ç†
                    logits = self.model(input_tensor)  # [1, seq_len, vocab_size]
                    
                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logitsç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ªtoken
                    next_token_logits = logits[0, -1, :]  # [vocab_size]

                    # è¯­æ³•çº¦æŸ/åç½®ï¼ˆå‡å°‘è¿ç»­æ§åˆ¶tokenï¼Œæå‡Pitch/Durationï¼‰
                    next_token_logits = self._apply_syntax_biases(generated_sequence, next_token_logits)
                    
                    # åº”ç”¨æ¸©åº¦
                    next_token_logits = next_token_logits / temperature
                    
                    # Top-ké‡‡æ ·
                    if top_k > 0:
                        top_k_logits, top_k_indices = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                        
                        if top_p < 1.0:
                            # Top-p (nucleus) é‡‡æ ·
                            probs = torch.softmax(top_k_logits, dim=-1)
                            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                            
                            # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„tokens
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                            sorted_indices_to_remove[0] = 0
                            
                            # æ„å»ºæœ€ç»ˆçš„æ¦‚ç‡åˆ†å¸ƒ
                            final_probs = sorted_probs.clone()
                            final_probs[sorted_indices_to_remove] = 0
                            
                            if final_probs.sum() > 0:
                                final_probs = final_probs / final_probs.sum()
                                # é‡‡æ ·
                                sampled_sorted_idx = torch.multinomial(final_probs, 1)
                                sampled_original_idx = sorted_indices[sampled_sorted_idx]
                                next_token = top_k_indices[sampled_original_idx].item()
                            else:
                                # å›é€€åˆ°top-ké‡‡æ ·
                                probs = torch.softmax(top_k_logits, dim=-1)
                                sampled_idx = torch.multinomial(probs, 1)
                                next_token = top_k_indices[sampled_idx].item()
                        else:
                            # åªä½¿ç”¨top-k
                            probs = torch.softmax(top_k_logits, dim=-1)
                            sampled_idx = torch.multinomial(probs, 1)
                            next_token = top_k_indices[sampled_idx].item()
                    else:
                        # è´ªå¿ƒè§£ç 
                        next_token = torch.argmax(next_token_logits).item()
                    
                    # æ·»åŠ ç”Ÿæˆçš„token
                    generated_sequence.append(next_token)
                    
                    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸç¬¦
                    if next_token == eos_token:
                        print(f"   é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                        break
                    
                    # é¿å…ç”Ÿæˆè¿‡å¤špadding tokens
                    if next_token == pad_token:
                        consecutive_pads = 0
                        for i in range(len(generated_sequence) - 1, -1, -1):
                            if generated_sequence[i] == pad_token:
                                consecutive_pads += 1
                            else:
                                break
                        if consecutive_pads >= 3:
                            print(f"   é‡åˆ°è¿ç»­paddingï¼Œåœæ­¢ç”Ÿæˆ")
                            break
                
                # è¿”å›ç”Ÿæˆçš„å®Œæ•´åºåˆ—
                # ç”Ÿæˆååšä¸€æ¬¡sanitizerï¼Œæå‡PerTokè§£ç æˆåŠŸç‡
                sanitized = self._sanitize_tokens(generated_sequence)
                print(f"   ç”ŸæˆæˆåŠŸ: {len(sanitized)} ä¸ªtokens (æ–°å¢{len(sanitized) - len(input_tokens)}ä¸ª)")
                return sanitized
                
        except Exception as e:
            print(f"âŒ è£…é¥°éŸ³ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def decode_to_midi(self, tokens, output_path: str):
        """è§£ç tokensä¸ºMIDIæ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨PerTokå†…éƒ¨è§£ç ï¼‰"""
        try:
            print(f"ğŸ¼ è§£ç ä¸ºMIDI: {output_path}")
            print(f"   è§£ç tokensæ•°é‡: {len(tokens)}")

            # è§£ç å‰è¿›è¡Œè¯­æ³•æ¸…æ´—
            tokens = self._sanitize_tokens(tokens)

            # ä½¿ç”¨å¢å¼ºçš„FixedPerTokDecoderè¿›è¡Œè§£ç ï¼ˆåŒ…å«è¯¦ç»†è°ƒè¯•ï¼‰
            score = self.decoder.decode_tokens(tokens)

            # ä¸¥æ ¼æ¨¡å¼ä¸‹çš„æ£€æŸ¥ä¸PerTokå…¼å®¹è§£ç 
            if score is None or not hasattr(score, 'tracks') or len(score.tracks) == 0:
                if not self.allow_fallback:
                    print("âš ï¸  PerTokå†…éƒ¨è§£ç å¤±è´¥ï¼Œä½¿ç”¨PerTokå…¼å®¹æ‰‹åŠ¨è§£ç ...")
                    # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨è§£ç å™¨ï¼Œä½†å®ƒä¸¥æ ¼æŒ‰ç…§PerTok tokenæ ¼å¼è¿›è¡Œè§£æ
                    # è¿™åœ¨æŠ€æœ¯ä¸Šä»ç„¶æ˜¯"PerTokè§£ç "ï¼Œå› ä¸ºå®ƒä½¿ç”¨ç›¸åŒçš„tokenå®šä¹‰å’Œè¯­ä¹‰
                    score = self.decoder._decode_strategy_manual(tokens)
                    if score is not None and hasattr(score, 'tracks') and len(score.tracks) > 0:
                        track_count = len(score.tracks)
                        note_count = sum(len(t.notes) for t in score.tracks)
                        print(f"âœ… PerTokå…¼å®¹è§£ç æˆåŠŸ: {track_count}è½¨é“, {note_count}éŸ³ç¬¦")
                    else:
                        print("âŒ PerTokå…¼å®¹è§£ç å¤±è´¥ï¼Œä¸¥æ ¼æ¨¡å¼ç»ˆæ­¢")
                        return False
                else:
                    print("âš ï¸  æ‰€æœ‰è§£ç ç­–ç•¥å¤±è´¥")
                    return False

            if score is None or not hasattr(score, 'tracks') or len(score.tracks) == 0:
                print("âŒ è§£ç å¤±è´¥: æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„Score")
                return False

            # ä¿å­˜ä¸ºMIDIæ–‡ä»¶
            # ä¼˜å…ˆè°ƒç”¨ symusic çš„ dump_midiï¼ˆscore å¯èƒ½æ˜¯ symusic.Score/ScoreTickï¼‰
            try:
                score.dump_midi(output_path)
            except Exception:
                # è‹¥ä¸ºå›é€€å¯¹è±¡ï¼Œä½¿ç”¨ä¿®å¤è§£ç å™¨ä¿å­˜
                if not self.decoder.save_to_midi(score, output_path):
                    print("âŒ MIDIä¿å­˜å¤±è´¥")
                    return False

            file_size = os.path.getsize(output_path)
            total_notes = sum(len(track.notes) for track in getattr(score, 'tracks', []))
            print(f"âœ… MIDIä¿å­˜æˆåŠŸ:")
            print(f"   æ–‡ä»¶: {output_path} ({file_size} bytes)")
            print(f"   è½¨é“æ•°: {len(getattr(score, 'tracks', []))}")
            print(f"   éŸ³ç¬¦æ•°: {total_notes}")
            return True

        except Exception as e:
            print(f"âŒ MIDIè§£ç å¤±è´¥: {e}")
            return False
    
    # ---------------- internal helpers ----------------
    def _build_token_sets(self):
        vocab = self.tokenizer.vocab
        self._ids_pitch = {i for s, i in vocab.items() if s.startswith('Pitch_')}
        self._ids_velocity = {i for s, i in vocab.items() if s.startswith('Velocity_')}
        self._ids_duration = {i for s, i in vocab.items() if s.startswith('Duration_')}
        self._ids_timeshift = {i for s, i in vocab.items() if s.startswith('TimeShift_')}
        self._ids_micro = {i for s, i in vocab.items() if s.startswith('MicroTiming_')}
        self._ids_timesig = {i for s, i in vocab.items() if s.startswith('TimeSig_')}
        self._ids_special = {i for s, i in vocab.items() if s.endswith('_None')}
        # å…è®¸çš„tokené›†åˆï¼ˆè§£ç ä¸¥æ ¼ï¼‰
        self._ids_allowed = set().union(
            self._ids_pitch,
            self._ids_velocity,
            self._ids_duration,
            self._ids_timeshift,
            self._ids_micro,
            self._ids_timesig,
            self._ids_special,
        )
        # å¸¸ç”¨TimeSigä¼˜å…ˆID
        self._id_timesig_44 = next((i for s, i in vocab.items() if s == 'TimeSig_4/4'), None)

    def _last_non_control(self, seq):
        for t in reversed(seq):
            if t not in self._ids_timeshift and t not in self._ids_micro:
                return t
        return None

    def _apply_syntax_biases(self, seq, logits):
        """å¯¹ä¸‹ä¸€æ­¥ logits æ–½åŠ ç®€å•è¯­æ³•åç½®ï¼Œå‡å°‘ä¼‘æ­¢/ç©ºç™½å¹¶æå‡Pitch/Durationæ¦‚ç‡ã€‚"""
        with torch.no_grad():
            bias = torch.zeros_like(logits)
            last_tok = self._last_non_control(seq)

            # é€šç”¨ï¼šé™ä½è¿ç»­ TimeShift/MicroTiming æ¦‚ç‡
            if len(seq) >= 1 and (seq[-1] in self._ids_timeshift or seq[-1] in self._ids_micro):
                bias[list(self._ids_timeshift)] -= 1.0
                bias[list(self._ids_micro)] -= 0.5

            # è‹¥ä¸Šä¸€ä¸ªéæ§åˆ¶ä¸æ˜¯ Pitchï¼Œåˆ™æ›´å¸Œæœ›ä¸‹ä¸€æ­¥æ˜¯ Pitch
            if last_tok is None or last_tok not in self._ids_pitch:
                bias[list(self._ids_pitch)] += 0.9
                # åŒæ—¶æŠ‘åˆ¶ç»§ç»­TimeShift
                bias[list(self._ids_timeshift)] -= 0.7
            else:
                # ä¸Šä¸€ä¸ªæ˜¯ Pitchï¼šä¸‹ä¸€æ­¥é¼“åŠ± Velocity æˆ– Durationï¼ˆä¼˜å…ˆç»™å‡ºæ—¶å€¼/åŠ›åº¦ï¼‰
                bias[list(self._ids_velocity)] += 0.5
                bias[list(self._ids_duration)] += 0.8

            # è½¯çº¦æŸï¼šæ§åˆ¶ç±»tokenæ•´ä½“è½»åº¦é™æƒ
            bias[list(self._ids_micro)] -= 0.2

            return logits + bias

    def _sanitize_tokens(self, tokens):
        """æ¸…æ´—ç”Ÿæˆåºåˆ—ä»¥æ»¡è¶³ PerTok è¯­æ³•ï¼š
        - ç¡®ä¿å¼€å¤´æœ‰ TimeSig æˆ– BOSï¼ˆè‹¥å­˜åœ¨ï¼‰
        - åˆå¹¶è¿ç»­ TimeShift/MicroTimingï¼ˆä»…ä¿ç•™ä¸€ä¸ªï¼‰
        - ä¸ºæ—  Duration çš„ Pitch è¡¥é»˜è®¤ Duration
        - å»é™¤ç»“å°¾å¤šä½™çš„æ§åˆ¶ç±» token
        """
        vocab = self.tokenizer.vocab
        id_to_str = {v: k for k, v in vocab.items()}

        def default_duration_id():
            # é€‰æ‹©ä¸€ä¸ªå¸¸è§æ—¶å€¼ï¼ˆ0.80.. çº¦åŠæ‹/æ‹ï¼‰ï¼Œå›é€€åˆ°ç¬¬ä¸€ä¸ªDuration
            for tid in self._ids_duration:
                s = id_to_str.get(tid, '')
                if s.startswith('Duration_0.8') or s.startswith('Duration_1'):
                    return tid
            return next(iter(self._ids_duration)) if self._ids_duration else None

        # ä»…ä¿ç•™å…è®¸çš„tokenï¼Œé¿å…PerTokæ— æ³•è§£æçš„â€œotherâ€ç±»
        filtered = [t for t in tokens if t in self._ids_allowed]

        out = []
        # è‹¥æœ‰ TimeSigï¼Œç¡®ä¿æ”¾åœ¨æœ€å‰
        seen_content = False
        for t in filtered:
            if not seen_content and t in self._ids_timesig:
                out.append(t)
                continue
            if t not in self._ids_timeshift and t not in self._ids_micro and t not in self._ids_special:
                seen_content = True
            out.append(t)

        # è‹¥å¼€å¤´æ²¡æœ‰TimeSigä¸”è¯è¡¨æœ‰4/4ï¼Œæ’å…¥ä¸€æš
        if (not out) or (out[0] not in self._ids_timesig):
            if self._id_timesig_44 is not None:
                out.insert(0, self._id_timesig_44)

        # åˆå¹¶è¿ç»­ TimeShift/MicroTiming
        merged = []
        prev_is_ts = False
        prev_is_micro = False
        for t in out:
            if t in self._ids_timeshift:
                if prev_is_ts:
                    continue
                prev_is_ts = True
                prev_is_micro = False
            elif t in self._ids_micro:
                if prev_is_micro:
                    continue
                prev_is_micro = True
                prev_is_ts = False
            else:
                prev_is_ts = prev_is_micro = False
            merged.append(t)

        # äº‹ä»¶çº§é‡æ’ï¼šç¡®ä¿ [Pitch][Velocity?][MicroTiming?][Duration] çš„é¡ºåº
        cleaned = []
        i = 0
        dur_id = default_duration_id()
        while i < len(merged):
            t = merged[i]
            if t in self._ids_pitch:
                pitch_tok = t
                vel_tok = None
                micro_tok = None
                dur_tok = None
                look_end = min(i + 8, len(merged))
                k = i + 1
                while k < look_end:
                    tk = merged[k]
                    if tk in self._ids_velocity and vel_tok is None:
                        vel_tok = tk
                    elif tk in self._ids_micro and micro_tok is None:
                        micro_tok = tk
                    elif tk in self._ids_duration and dur_tok is None:
                        dur_tok = tk
                    elif tk in self._ids_pitch or tk in self._ids_timeshift:
                        break
                    k += 1
                cleaned.append(pitch_tok)
                if vel_tok is not None:
                    cleaned.append(vel_tok)
                if micro_tok is not None:
                    cleaned.append(micro_tok)
                cleaned.append(dur_tok if dur_tok is not None else dur_id)
                # è·³è¿‡å·²æ¶ˆè´¹çš„çª—å£
                i = k
                continue
            else:
                cleaned.append(t)
                i += 1

        # å»é™¤ç»“å°¾å¤šä½™æ§åˆ¶ token
        while cleaned and (cleaned[-1] in self._ids_timeshift or cleaned[-1] in self._ids_micro):
            cleaned.pop()

        # è‹¥æœ‰ BOS/EOS åˆ™è¡¥é½ï¼ˆæœ€åå†è¡¥ï¼‰
        bos = vocab.get('BOS_None')
        eos = vocab.get('EOS_None')
        if bos is not None and (len(cleaned) == 0 or cleaned[0] != bos):
            cleaned.insert(0, bos)
        if eos is not None and cleaned[-1] != eos:
            cleaned.append(eos)

        return cleaned
    def analyze_ornament_content(self, tokens):
        """åˆ†ætokenåºåˆ—ä¸­çš„è£…é¥°éŸ³å†…å®¹"""
        try:
            analyzer = self.ornament_loss.analyzer
            
            analysis = {
                'total_tokens': len(tokens),
                'ornament_tokens': 0,
                'ornament_ratio': 0.0,
                'ornament_categories': {},
                'average_weight': 0.0
            }
            
            weights = []
            for token_id in tokens:
                if token_id < len(self.tokenizer.vocab):
                    weight = analyzer.get_ornament_weight(token_id)
                    weights.append(weight)
                    
                    if analyzer.is_ornament_token(token_id):
                        analysis['ornament_tokens'] += 1
                        
                        # ç»Ÿè®¡è£…é¥°éŸ³ç±»åˆ«
                        for category, token_set in analyzer.ornament_tokens.items():
                            if token_id in token_set:
                                analysis['ornament_categories'][category] = analysis['ornament_categories'].get(category, 0) + 1
            
            if analysis['total_tokens'] > 0:
                analysis['ornament_ratio'] = analysis['ornament_tokens'] / analysis['total_tokens']
                analysis['average_weight'] = sum(weights) / len(weights) if weights else 1.0
            
            return analysis
            
        except Exception as e:
            print(f"âŒ è£…é¥°éŸ³åˆ†æå¤±è´¥: {e}")
            return {}
    
    def generate_from_midi(self, input_path: str, output_path: str, **kwargs):
        """å®Œæ•´çš„MIDIåˆ°MIDIè£…é¥°éŸ³ç”Ÿæˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´è£…é¥°éŸ³ç”Ÿæˆæµç¨‹")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. ç¼–ç è¾“å…¥MIDI
        print("\n1ï¸âƒ£ ç¼–ç è¾“å…¥MIDI")
        input_tokens = self.encode_midi(input_path)
        if input_tokens is None:
            print("âŒ æµç¨‹ç»ˆæ­¢ï¼šMIDIç¼–ç å¤±è´¥")
            return False
        
        # 2. åˆ†æè¾“å…¥å†…å®¹
        print("\n2ï¸âƒ£ åˆ†æè¾“å…¥å†…å®¹")
        input_analysis = self.analyze_ornament_content(input_tokens)
        print(f"   æ€»tokens: {input_analysis['total_tokens']}")
        print(f"   è£…é¥°éŸ³tokens: {input_analysis['ornament_tokens']} ({input_analysis['ornament_ratio']:.1%})")
        print(f"   å¹³å‡æƒé‡: {input_analysis['average_weight']:.2f}")
        
        # 3. ç”Ÿæˆè£…é¥°éŸ³
        print("\n3ï¸âƒ£ ç”Ÿæˆè£…é¥°éŸ³")
        generated_tokens = self.generate_ornaments(input_tokens, **kwargs)
        if generated_tokens is None:
            print("âŒ æµç¨‹ç»ˆæ­¢ï¼šè£…é¥°éŸ³ç”Ÿæˆå¤±è´¥")
            return False
        
        # 4. åˆ†æç”Ÿæˆå†…å®¹
        print("\n4ï¸âƒ£ åˆ†æç”Ÿæˆå†…å®¹")
        output_analysis = self.analyze_ornament_content(generated_tokens)
        print(f"   æ€»tokens: {output_analysis['total_tokens']}")
        print(f"   è£…é¥°éŸ³tokens: {output_analysis['ornament_tokens']} ({output_analysis['ornament_ratio']:.1%})")
        print(f"   å¹³å‡æƒé‡: {output_analysis['average_weight']:.2f}")
        
        # 5. è§£ç ä¸ºMIDI
        print("\n5ï¸âƒ£ è§£ç ä¸ºMIDI")
        decode_success = self.decode_to_midi(generated_tokens, output_path)
        if not decode_success:
            print("âŒ æµç¨‹ç»ˆæ­¢ï¼šMIDIè§£ç å¤±è´¥")
            return False
        
        # 6. æ€»ç»“
        elapsed_time = time.time() - start_time
        ornament_enhancement = output_analysis['ornament_ratio'] - input_analysis['ornament_ratio']
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æµç¨‹å®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"âœ… è¾“å…¥æ–‡ä»¶: {input_path}")
        print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.1f}ç§’")
        print(f"ğŸµ è¾“å…¥è£…é¥°éŸ³: {input_analysis['ornament_ratio']:.1%}")
        print(f"ğŸ¨ è¾“å‡ºè£…é¥°éŸ³: {output_analysis['ornament_ratio']:.1%}")
        print(f"ğŸ“ˆ è£…é¥°éŸ³å¢å¼º: {ornament_enhancement:+.1%}")
        
        if ornament_enhancement > 0:
            print("\nğŸ‰ è£…é¥°éŸ³ç”ŸæˆæˆåŠŸå®Œæˆï¼")
        else:
            print("\nâš ï¸  æ³¨æ„ï¼šç”Ÿæˆçš„è£…é¥°éŸ³æ¯”ä¾‹æœªå¢åŠ ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç”Ÿæˆå‚æ•°")
        
        return True


def main():
    parser = argparse.ArgumentParser(
        description='è£…é¥°éŸ³ç”Ÿæˆæ¨ç† - Leveraging PerTok and Domain-Specific Transformer Design for Expressive MIDI Ornament Generation',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€ç”Ÿæˆ
  python inference.py --input examples/bach.mid --output output/bach_ornament.mid
  
  # è°ƒæ•´ç”Ÿæˆå‚æ•°
  python inference.py --input input.mid --output output.mid --temperature 1.2 --top_k 50 --top_p 0.9
  
  # ä½¿ç”¨ç‰¹å®šæ¨¡å‹
  python inference.py --model checkpoints_ornament_aware/best_ornament_aware_model.pth --input input.mid --output output.mid
        """)
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--input', '-i', type=str, required=True,
                        help='è¾“å…¥MIDIæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, required=True,
                        help='è¾“å‡ºMIDIæ–‡ä»¶è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', '-m', type=str, 
                        default='checkpoints_ornament_aware/best_ornament_aware_model.pth',
                        help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: æœ€æ–°çš„OrnamentAwareæ¨¡å‹)')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'cuda', 'cpu'],
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--allow_fallback', action='store_true', default=False,
                        help='å…è®¸åœ¨PerTokè§£ç å¤±è´¥æ—¶å›é€€åˆ°æ‰‹å†™è§£ç ï¼ˆé»˜è®¤ä¸¥æ ¼æ¨¡å¼ï¼šä¸å›é€€ï¼‰')
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument('--temperature', '-t', type=float, default=1.1,
                        help='ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 1.1, èŒƒå›´: 0.5-2.0)')
    parser.add_argument('--top_k', type=int, default=50,
                        help='Top-ké‡‡æ · (é»˜è®¤: 50)')
    parser.add_argument('--top_p', type=float, default=0.9,
                        help='Top-pé‡‡æ · (é»˜è®¤: 0.9)')
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--force', '-f', action='store_true',
                        help='å¼ºåˆ¶è¦†ç›–è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return 1
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(args.output) and not args.force:
        response = input(f"âš ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨: {args.output}\næ˜¯å¦è¦†ç›–? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("âŒ æ“ä½œå–æ¶ˆ")
            return 1
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²è®­ç»ƒæ¨¡å‹æˆ–æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return 1
    
    # å‚æ•°éªŒè¯
    if not (0.1 <= args.temperature <= 3.0):
        print(f"âŒ æ¸©åº¦å‚æ•°è¶…å‡ºèŒƒå›´: {args.temperature} (åº”åœ¨0.1-3.0ä¹‹é—´)")
        return 1
    
    if not (1 <= args.top_k <= 200):
        print(f"âŒ top_kå‚æ•°è¶…å‡ºèŒƒå›´: {args.top_k} (åº”åœ¨1-200ä¹‹é—´)")
        return 1
    
    if not (0.1 <= args.top_p <= 1.0):
        print(f"âŒ top_på‚æ•°è¶…å‡ºèŒƒå›´: {args.top_p} (åº”åœ¨0.1-1.0ä¹‹é—´)")
        return 1
    
    try:
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        engine = OrnamentInferenceEngine(args.model, args.device, allow_fallback=args.allow_fallback)
        
        # ç”Ÿæˆè£…é¥°éŸ³
        success = engine.generate_from_midi(
            input_path=args.input,
            output_path=args.output,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p
        )
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
